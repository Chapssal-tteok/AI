# app/api/interview/route.py

from fastapi import APIRouter, Body
from pydantic import BaseModel
from typing import List, Dict
from app.services.gpt_service import get_chat_response
from app.services.perplexity_service import search_perplexity_summary
from app.prompts.resume_analyze_prompts import generate_resume_analysis_prompt
from app.prompts.analyze_answer_prompts import analyze_answer_prompt
from app.prompts.follow_up_prompts import generate_follow_up_prompt
from app.prompts.interview_qas_prompts import generate_interview_qas_prompt

router = APIRouter()

class ResumeRequest(BaseModel):
    resume: str
    company: str
    position: str

class AnswerAnalysisRequest(BaseModel):
    question: str
    answer: str
    resume: str

class FollowUpRequest(BaseModel):
    question: str
    answer: str

class InterviewQasRequest(BaseModel):
    company: str
    position: str
    resumeContent: str


@router.post(
    "/analyze-resume",
    summary="ìê¸°ì†Œê°œì„œ ë¶„ì„",
    description="ìê¸°ì†Œê°œì„œì™€ ì§€ì› ì •ë³´(ê¸°ì—…, ì§ë¬´)ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
    response_model=Dict[str, List[str]],
    responses={
        200: {
            "description": "ë¶„ì„ëœ í”¼ë“œë°± ëª©ë¡ ë°˜í™˜",
            "content": {
                "application/json": {
                    "example": {
                        "feedback": [
                            "**ë…¼ë¦¬ì  íë¦„**: ìê¸°ì†Œê°œì„œì˜ ì „ì²´ íë¦„ì´ ë¶€ë“œëŸ½ê³  ìì—°ìŠ¤ëŸ¬ì›Œìš”.",
                            "**êµ¬ì²´ì„±**: ê²½í—˜ì´ êµ¬ì²´ì ìœ¼ë¡œ ì˜ ë“œëŸ¬ë‚˜ ìˆì–´ ì„¤ë“ë ¥ì´ ë†’ì•„ìš”.",
                            "**ê°œì„  í¬ì¸íŠ¸**: ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì˜ ì˜ˆì‹œëŠ” ë‹¤ì†Œ ëª¨í˜¸í•˜ë‹ˆ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê²°ê³¼ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”."
                        ]
                    }
                }
            }
        }
    }
)
async def analyze_resume(req: ResumeRequest = Body(..., example={
    "resume": "ì €ëŠ” ëŒ€í•™ ìƒí™œ ë™ì•ˆ ë‹¤ì–‘í•œ íŒ€ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ë©° í˜‘ì—… ëŠ¥ë ¥ì„ í‚¤ì› ìŠµë‹ˆë‹¤...",
    "company": "ë„¤ì´ë²„",
    "position": "ë°±ì—”ë“œ ê°œë°œì"
})):
    prompt = generate_resume_analysis_prompt(req.resume, req.company, req.position)
    response = get_chat_response(prompt, model="sonar", mode="text")

    if not response or not isinstance(response, str):
        return {"message": "ë¶„ì„ ì‹¤íŒ¨"}

    feedback = [line for line in response.split("\n") if line.strip()]
    return {"feedback": feedback}


@router.post(
    "/analyze-answer",
    summary="ë©´ì ‘ ë‹µë³€ ë¶„ì„",
    description="ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€, ìê¸°ì†Œê°œì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì˜ ê°•ì , ì•½ì , ê°œì„  í¬ì¸íŠ¸ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.",
    response_model=Dict[str, str],
    responses={
        200: {
            "description": "ë‹µë³€ ë¶„ì„ ê²°ê³¼ ë°˜í™˜",
            "content": {
                "application/json": {
                    "example": {
                        "analysis": "1. ê°•ì : ì§ˆë¬¸ ì˜ë„ë¥¼ ì˜ íŒŒì•…í•˜ê³  ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•¨..."
                    }
                }
            }
        }
    }
)
async def analyze_answer(req: AnswerAnalysisRequest = Body(..., example={
    "question": "ì–´ë ¤ìš´ ìƒí™©ì—ì„œ ê°ˆë“±ì„ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‚˜ìš”?",
    "answer": "ì €ëŠ” ë™ì•„ë¦¬ í”„ë¡œì íŠ¸ì—ì„œ ì¼ì •ì´ ì§€ì—°ëœ íŒ€ì›ê³¼ ê°ˆë“±ì„ ê²ªì€ ì  ìˆìŠµë‹ˆë‹¤...",
    "resume": "ì €ëŠ” ë‹¤ì–‘í•œ í˜‘ì—… í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ê°ˆë“± ì¡°ì • ëŠ¥ë ¥ì„ í‚¤ì› ìŠµë‹ˆë‹¤..."
})):
    prompt = analyze_answer_prompt(req.question, req.answer, req.resume)
    response = get_chat_response(prompt, model="sonar", mode="text")

    if not response or not isinstance(response, str):
        return {"message": "ë‹µë³€ ë¶„ì„ ì‹¤íŒ¨"}

    return {"analysis": response}

@router.post(
    "/follow-up",
    summary="ì¶”ê°€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±",
    description="ì§€ì›ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” ì¶”ê°€ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=Dict[str, List[str]],
    responses={
        200: {
            "description": "ì¶”ê°€ ì§ˆë¬¸ ëª©ë¡ ë°˜í™˜",
            "content": {
                "application/json": {
                    "example": {
                        "followUps": ["ë‹¹ì‹œ íŒ€ì›ê³¼ì˜ ê°ˆë“±ì„ í•´ê²°í•œ ê³¼ì •ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"]
                    }
                }
            }
        }
    }
)
async def generate_follow_up(req: FollowUpRequest = Body(..., example={
    "question": "ê°ˆë“±ì„ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‚˜ìš”?",
    "answer": "ë„¤, ì €ëŠ” í”„ë¡œì íŠ¸ì—ì„œ..."
})):
    prompt = generate_follow_up_prompt(req.question, req.answer)
    response = get_chat_response(prompt, model="sonar", mode="text")

    if not response or not isinstance(response, str):
        return {"message": "ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"}

    follow_ups = [line for line in response.split("\n") if line.strip()]
    return {"followUps": follow_ups}


@router.post(
    "/generate-qas",
    summary="ë©´ì ‘ ì§ˆë¬¸ ìƒì„±",
    description="ê¸°ì—…ê³¼ ì§ë¬´ ì •ë³´, ìê¸°ì†Œê°œì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=Dict[str, List[str]],
    responses={
        200: {
            "description": "ìƒì„±ëœ ë©´ì ‘ ì§ˆë¬¸ ëª©ë¡ ë°˜í™˜",
            "content": {
                "application/json": {
                    "example": {
                        "questions": [
                            "[ë„¤ì´ë²„] ë°±ì—”ë“œ ê°œë°œìë¡œì„œ ìµœê·¼ í”„ë¡œì íŠ¸ ì¤‘ ê°€ì¥ ë„ì „ì ì´ì—ˆë˜ ê²½í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                            "ìì‹ ì˜ ê¸°ìˆ  ìŠ¤íƒ ì¤‘ ë„¤ì´ë²„ì—ì„œ ê°€ì¥ ì˜ í™œìš©í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?"
                        ]
                    }
                }
            }
        }
    }
)
async def generate_interview_questions(req: InterviewQasRequest = Body(..., example={
    "company": "ë„¤ì´ë²„",
    "position": "ë°±ì—”ë“œ ê°œë°œì",
    "resumeContent": "ì €ëŠ” ëŒ€ê·œëª¨ íŠ¸ë˜í”½ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì„¤ê³„ë¥¼ ê²½í—˜í–ˆìŠµë‹ˆë‹¤..."
})):
    # 1. ê¸°ì—… + ì§ë¬´ë¡œ Perplexity ìš”ì•½ ê²€ìƒ‰
    search_query = f"{req.company} {req.position}"
    pplx_summary = search_perplexity_summary(search_query)

    # 2. ìš”ì•½ ê²°ê³¼ì™€ ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í•©ì³ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = generate_interview_qas_prompt(pplx_summary, req.resumeContent)

    print("ğŸ“¨ ìµœì¢… prompt:\n", prompt)
    print("ğŸ“ ê¸¸ì´:", len(prompt))
    
    # 3. GPTë¡œ ì§ˆë¬¸ ìƒì„±
    response = get_chat_response(prompt, model="sonar", mode="text")

    if not response or not isinstance(response, str):
        return {"message": "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"}

    questions = [line for line in response.split("\n") if line.strip()]
    return {"questions": questions}
